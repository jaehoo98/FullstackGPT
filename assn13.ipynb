{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챌린지\n",
    "(EN)\n",
    "Implement a complete RAG pipeline with a Stuff Documents chain.\n",
    "\n",
    "You must implement the chain manually.\n",
    "\n",
    "Give a ConversationBufferMemory to the chain.\n",
    "\n",
    "Use this document to perform RAG: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "\n",
    "Ask the following questions to the chain:\n",
    "\n",
    "Is Aaronson guilty?\n",
    "\n",
    "What message did he write in the table?\n",
    "\n",
    "Who is Julia?\n",
    "\n",
    "\n",
    "\n",
    "(KR)\n",
    "Stuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현하세요.\n",
    "\n",
    "체인을 수동으로 구현해야 합니다.\n",
    "\n",
    "체인에 ConversationBufferMemory를 부여합니다.\n",
    "\n",
    "이 문서를 사용하여 RAG를 수행하세요: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "\n",
    "체인에 다음 질문을 합니다:\n",
    "\n",
    "Aaronson 은 유죄인가요?\n",
    "\n",
    "그가 테이블에 어떤 메시지를 썼나요?\n",
    "\n",
    "Julia 는 누구인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='According to the text, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.'\n",
      "content='He wrote \"2+2=5\" in the dust on the table.'\n",
      "content='Julia is a character who was involved with the protagonist in the story.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": RunnableLambda(load_memory),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "questions = [\"Is Aaronson guilty?\",\n",
    "             \"What message did he write in the table?\",\n",
    "             \"Who is Julia?\",\n",
    "]\n",
    "for question in questions:\n",
    "    result = chain.invoke(question)\n",
    "    print(result)\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
